{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T00:04:14.046805Z",
     "start_time": "2024-07-05T00:04:13.635537Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Non-Negative Matrix Factorization for Movie Recommendations\n",
    "\n",
    "This appends to a previous week's assignment on recommender systems.\n",
    "\n",
    "We have four files:\n",
    "- rec_movies: one row per movie, its release year, and a number of columns for genre\n",
    "- rec_users: one row per user, their gender, their age, their occupation, and their zip code\n",
    "- rec_train: one row per user, movie, and rating\n",
    "- rec_test: same as rec_train\n",
    "\n",
    "The general idea here is that we start with the utility matrix with one row per user and one column per movies. The cells are ratings for that user and that movie. \n",
    "\n",
    "When we use non-negative matrix factorization (NMF), we decompose the utility matrix into the W and H matrices. The W matrix has one row per user. The H matrix has one row per movie. They share the same columns with each column representing a latent factor. A latent factor is an aspect of the data in the utility matrix encoded similar to how the word embeddings from Word2Vec created new meaningful features that we are not able to map one-to-one to something meaningful to us. NMF learns patterns in the utility matrix and encodes them in the features for W and H.\n",
    "\n",
    "We then multiply W and H together with these patterns encoded in the latent factors. The result of that multiplication is a new utility matrix with the same dimensions, row meanings, and column meanings as the original. The catch is that multiplying W and H tells us what each cell is expected to be based on extending the patterns encoded in the latent factors. This lets us replace 0s with what we would expect to see based on those latent factors, allowing us to fill in the 0s with predictions.\n",
    "\n",
    "From there, we can select a user and a movie we are interested in and look up the prediction in the new utility matrix. We loop through the test set and pull out predictions for each user and movie combination.\n",
    "\n",
    "Singular value decomposition (SVD) works differently in terms of how it decomposes the utility matrix. I explained SVD in the BBC classification notebook. For this notebook, I am using SVD to see how similar or different its RMSE ends up being, mostly for curiosity and reference for NMF."
   ],
   "id": "a8d0cf79081dd36e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Load Moving Ratings Data and Predict with Matrix Factorization\n",
   "id": "77eb76735069698e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T00:04:18.337088Z",
     "start_time": "2024-07-05T00:04:18.031918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "users = pd.read_csv('data/rec_users.csv')\n",
    "movies = pd.read_csv('data/rec_movies.csv')\n",
    "train = pd.read_csv('data/rec_train.csv')\n",
    "test = pd.read_csv('data/rec_test.csv')"
   ],
   "id": "fb278ba43732dda6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The trickiest code here is the creation of the utility matrix. We read in the training dataset with one row per user, movie, and rating. We need to convert that to a utility matrix with one row per user and one column per movie. Each cell will be that user's rating for that movie. We build the utility matrix using `scipy` sparse matrix functionality with `coo_matrix()`.",
   "id": "b23704e966f9f029"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T00:04:20.664355Z",
     "start_time": "2024-07-05T00:04:19.473074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "allusers = list(users['uID'])\n",
    "allmovies = list(movies['mID'])\n",
    "genres = list(movies.columns.drop(['mID', 'title', 'year']))\n",
    "mid2idx = dict(zip(movies.mID,list(range(len(movies)))))\n",
    "uid2idx = dict(zip(users.uID,list(range(len(users)))))\n",
    "\n",
    "# Turns the train set into a utility matrix with one row per user, one \n",
    "# column per movie, and each cell as that user's rating for that movie\n",
    "movie_ratings_utility_matrix = np.array(\n",
    "        coo_matrix(\n",
    "            (\n",
    "                list(train.rating)\n",
    "                , (\n",
    "                    [uid2idx[x] for x in train.uID]\n",
    "                    , [mid2idx[x] for x in train.mID]\n",
    "                )\n",
    "            ), shape=(\n",
    "                len(allusers)\n",
    "                , len(allmovies)\n",
    "            )\n",
    "        ).toarray()\n",
    "    )"
   ],
   "id": "c8554e1e0ea8a333",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As a reminder, root mean squared error (RMSE) takes the difference between each prediction and the actual value, squares that distance, find the mean of those squared distances, and then takes the square root of that mean. This gives us a measure of how far apart on average each prediction is from the corresponding observed value. Squaring the mean puts the RMSE measure back into the scale of the units of ratings, with the ratings being on a 0-5 integer scale.",
   "id": "81a1d6d9fd6e69f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T00:04:23.108226Z",
     "start_time": "2024-07-05T00:04:23.104822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rmse(y_preds):\n",
    "    y_preds[np.isnan(y_preds)] = 3 #In case there is nan values in prediction, it will impute to 3.\n",
    "    y_true = np.array(test.rating)\n",
    "    return np.sqrt(((y_true - y_preds)**2).mean())"
   ],
   "id": "1eeccb9a0070961a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Not including solutions here, but tested and confirmed that \n",
    "- `predict_everything_to_3()`\n",
    "- `predict_to_user_average()`\n",
    "\n",
    "works. Just to make sure my refactoring here did not mess anything up.\n",
    "\n",
    "As a reminder, here is how each of the methods performed in the week three assignment.\n",
    "\n",
    "| Method                              | RMSE  |\n",
    "|:------------------------------------|:-----:|\n",
    "| Baseline, $Y_p$=3                   | 1.259 |\n",
    "| Baseline, $Y_p=\\mu_u$               | 1.035 |\n",
    "| Content based, item-item            | 1.013 |\n",
    "| Collaborative, cosine               | 1.026 |\n",
    "| Collaborative, jaccard, $M_r\\geq 3$ | 0.982 |\n",
    "| Collaborative, jaccard, $M_r\\geq 1$ | 0.991 |\n",
    "| Collaborative, jaccard, $M_r$       | 0.952 |"
   ],
   "id": "362e60444374df5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will test out both NMF and SVD. The assignment asks to test out NMF, but I was curious how NMF compares to another matrix factorization algorithm.\n",
    "\n",
    "We will need to toggle `max_iter` on for NMF after 15 components since the convergence for the estimated W and H matrices needs more than the default 200 iterations.\n",
    "\n",
    "We test out both NMF and SVD with different numbers of features in the decomposition spanning from 5 to 100. RMSE summaries for each are below."
   ],
   "id": "e2513c065ed030ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T00:04:47.071731Z",
     "start_time": "2024-07-05T00:04:38.812227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nmf = NMF(\n",
    "    n_components=5\n",
    "    # , max_iter=500\n",
    "    , random_state=42\n",
    ")\n",
    "W = nmf.fit_transform(movie_ratings_utility_matrix)\n",
    "H = nmf.components_\n",
    "\n",
    "preds_nmf = W.dot(H)"
   ],
   "id": "dce0eb32a8f1a6eb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T16:31:42.878666Z",
     "start_time": "2024-07-04T16:31:42.869535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'W matrix shape: {W.shape}')\n",
    "print(f'H matrix shape: {H.shape}')\n",
    "print(f'Utility matrix shape: {movie_ratings_utility_matrix.shape}')"
   ],
   "id": "8f5a84aaf52a0dd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W matrix shape: (6040, 25)\n",
      "H matrix shape: (25, 3883)\n",
      "Utility matrix shape: (6040, 3883)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T00:04:53.984960Z",
     "start_time": "2024-07-05T00:04:50.408326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "svd = TruncatedSVD(\n",
    "    n_components=5\n",
    "    , random_state=42\n",
    ")\n",
    "\n",
    "U = svd.fit_transform(movie_ratings_utility_matrix)\n",
    "S = np.diag(svd.singular_values_)\n",
    "V = svd.components_\n",
    "\n",
    "preds_svd = U.dot(S.dot(V))"
   ],
   "id": "59865cf8a5ba08ac",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T00:05:33.440699Z",
     "start_time": "2024-07-05T00:05:33.411443Z"
    }
   },
   "cell_type": "code",
   "source": "np.min(preds_nmf)",
   "id": "cc2228bdfd071e8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T16:31:46.857093Z",
     "start_time": "2024-07-04T16:31:46.851491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'U matrix shape: {U.shape}')\n",
    "print(f'S matrix shape: {S.shape}')\n",
    "print(f'V matrix shape: {V.shape}')\n",
    "print(f'Utility matrix shape: {movie_ratings_utility_matrix.shape}')"
   ],
   "id": "1c3a0a72b293935a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U matrix shape: (6040, 25)\n",
      "S matrix shape: (25, 25)\n",
      "V matrix shape: (25, 3883)\n",
      "Utility matrix shape: (6040, 3883)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NMF and SVD are returning predictions outside the original 0-5 rating scale. They are also producing floats. We will rescale both back to 0-5 and then round the predictions to get integer ratings.",
   "id": "97330f4f119bec9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T16:31:48.063279Z",
     "start_time": "2024-07-04T16:31:46.859998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mms_nmf = MinMaxScaler((0, 5))\n",
    "preds_test_nmf = np.array([\n",
    "    preds_nmf[uid2idx[uid], mid2idx[mid]]\n",
    "    for uid, mid in np.array(test.drop('rating', axis=1))\n",
    "])\n",
    "preds_test_nmf_scaled = mms_nmf.fit_transform(preds_test_nmf.reshape(-1,1)).flatten()\n",
    "preds_test_nmf_scaled_rounded = np.round(preds_test_nmf_scaled)\n",
    "\n",
    "mms_svd = MinMaxScaler((0, 5))\n",
    "preds_test_svd = np.array([\n",
    "    preds_svd[uid2idx[uid], mid2idx[mid]]\n",
    "    for uid, mid in np.array(test.drop('rating', axis=1))\n",
    "])\n",
    "preds_test_svd_scaled = mms_svd.fit_transform(preds_test_svd.reshape(-1,1)).flatten()\n",
    "preds_test_svd_scaled_rounded = np.round(preds_test_svd_scaled)"
   ],
   "id": "ea1463a324ed8d63",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T16:31:48.094642Z",
     "start_time": "2024-07-04T16:31:48.065336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'RMSE for NMF: {rmse(preds_test_nmf)}')\n",
    "print(f'RMSE for SVD: {rmse(preds_test_svd)}')\n",
    "print(f'RMSE for NMF scaled: {rmse(preds_test_nmf_scaled)}')\n",
    "print(f'RMSE for SVD scaled: {rmse(preds_test_svd_scaled)}')\n",
    "print(f'RMSE for NMF scaled and rounded: {rmse(preds_test_nmf_scaled_rounded)}')\n",
    "print(f'RMSE for SVD scaled and rounded: {rmse(preds_test_svd_scaled_rounded)}')"
   ],
   "id": "4708f220ff8d13c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for NMF: 2.8560903217001954\n",
      "RMSE for SVD: 1316.2845908671795\n",
      "RMSE for NMF scaled: 3.0760606462814613\n",
      "RMSE for SVD scaled: 3.1834367157206906\n",
      "RMSE for NMF scaled and rounded: 3.1341820543163386\n",
      "RMSE for SVD scaled and rounded: 3.2588719543608233\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RMSE Summary\n",
    "\n",
    "|       Algorithm        | Components: 5 | Components: 10 | Components: 15 | Components: 20 | Components: 25 & max_iter: 500 | Components: 50 & max_iter: 500 | Components: 100 & max_iter: 500 |\n",
    "|:----------------------:|:--------------|:---------------|:---------------|:---------------|:-------------------------------|:-------------------------------|:--------------------------------|\n",
    "|          NMF           | 2.991         | 2.912          | 2.873          | 2.862          | 2.856                          | 2.899                          | 3.008                           |\n",
    "|          SVD           | 1296.983      | 1315.453       | 1318.607       | 1318.425       | 1316.285                       | 1295.586                       | 1266.619                        |\n",
    "|       NMF Scaled       | 3.279         | 3.195          | 3.118          | 3.076          | 3.076                          | 3.076                          | 3.213                           |\n",
    "|       SVD Scaled       | 3.263         | 3.213          | 3.199          | 3.19           | 3.183                          | 3.167                          | 3.171                           |\n",
    "| NMF Scaled and Rounded | 3.362         | 3.266          | 3.181          | 3.134          | 3.134                          | 3.132                          | 3.273                           |\n",
    "| SVD Scaled and Rounded | 3.351         | 3.294          | 3.278          | 3.266          | 3.259                          | 3.241                          | 3.243                           |\n",
    "\n"
   ],
   "id": "727dd0fd805caa9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Why the Predictions Did Not Work\n",
    "\n",
    "The matrix factorization performance is not great. The best NMF or SVD RMSE is 2-3 times the worst RMSE from the previous attempts, the approach where we predicted everything to 3. The RMSE for NMF gets slightly better as we increase the number of components, but then it starts to drop again.\n",
    "\n",
    "The best NMF RMSE is 3.132. This means that on average each NMF prediction is 3.132 away from the corresponding observed value. On a 0-5 scale, that means we are more than half of the scale away on average for each prediction. This seems like a pretty poor value for RMSE. On a 0-5 scale, we would not want to trust these predictions.\n",
    "\n",
    "SVD does not perform much better. For different numbers of components, SVD is sometimes higher and sometimes lower than NMF, but it is also worse than the broad attempt at predicting everything to 3.\n",
    "\n",
    "The next question is why the NMF RMSE is so high.\n",
    "\n",
    "What I think is going on is that we are working with very sparse data and that most of the utility matrix is 0s, 0s representing missing data. We are asking NMF to decompose that sparse matrix without having enough information to go on due to the sparsity. Also, NMF is treating 0s as rating values, as seen by 0s showing up as predictions. This results in NMF trying to pick up on patterns in the data that we do not have enough data to clearly define along with the zeros skewing the decomposition. We see some evidence that something is off with the predictions in the out-of-range values. NMF predicted ratings higher than 8 when the input ratings were on the 0-5 scale.\n",
    "\n",
    "I am not sure how many component are typical before you need to up the `max_iter` parameter for NMF, but we had to do that around 15 latent features. 15 is lower than I was expecting. I think this is more evidence that NMF is struggling to decompose the utility matrix and needs more iterations available to estimate the W and H matrix approximations.\n",
    "\n",
    "So, in the end, we can decompose the utility matrix into W and H, but the latent factors are not accurate enough due to the sparsity of the original utility matrix. When we multiply W and H together to rebuild the utility matrix with predictions in place of 0s, those predictions are all over the place, on average 3.132 away from what they should be on a 0-5 scale.\n",
    "\n",
    "Here are some thoughts on addressing the sparsity issue.\n",
    "\n",
    "We could reduce the dimensions of the utility matrix so that it is not quite as sparse. This might mean that we trim the users or the movies that we run at once. We may have ways to segment users into different groups and see if we end up with less sparsity within any of those groups. Similar for finding subsets of movies. We also could look at something like regularization on the original feature space to help identify in a more statistical manner rather than domain-knowledge manner which movies to leave out.\n",
    "\n",
    "We could impute some values in place of 0s where appropriate, possibly by using domain knowledge or another form of prediction that is not as sweeping across the entire utility matrix. For example, we may be able to identify certain users or movies where we see stronger patterns, and we can use those to impute missing values with more certainty than estimating all missing values across the whole utility matrix at once.\n",
    "\n",
    "Both of these approaches assume that we can use a subset of the data to refine patterns and then apply those patterns to the full data. The assumption in there is that the subsets are similar enough to the full dataset that the patterns are relevant to all the data. This may or may not be true. It would take some digging into when trying to find subsets or smaller scopes for imputation.\n",
    "\n",
    "One last thought is to use algorithms that are better able to handle sparse matrices. One example would be to adjust the objective function for finding the W and H approximations. We might be able to adjust that to do something like penalize too many 0s or provide stronger weighting to non-0s. It does look like there are ways to incorporate constraints on the objective function. In a similar vein, we could look at algorithms that tackle the sparse matrix in a divide-and-conquer manner or similar that is able to ensemble together findings from subsets of the data that provide stronger signals for patterns in the data. This fits somewhat with what I mentioned a few paragraphs above, but there are also entire strategies that look like they are to address this issue of sparsity for matrix factorization.\n",
    "\n",
    "In summary, using the base NMF from `sklearn` as we did here is not a good solution for this set of movie ratings. My sense is that it is because of the sparsity of the utility matrix. For now, we either need to find an alternative matrix factorization approach or use one of the similarity-based measures that we implemented in the week three assignment."
   ],
   "id": "ca05d2e2b7a89a0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "I needed some help figuring this out. Here are some resources I used when getting up to speed on using matrix factorization for rating predictions.\n",
    "\n",
    "Matrix factorization for predictions:\n",
    "- https://medium.com/beek-tech/predicting-ratings-with-matrix-factorization-methods-cf6c68da775\n",
    "- https://medium.com/analytics-vidhya/matrix-factorization-made-easy-recommender-systems-7e4f50504477\n",
    "- https://medium.com/logicai/non-negative-matrix-factorization-for-recommendation-systems-985ca8d5c16c\n",
    "- https://stackoverflow.com/questions/42357450/scikit-learn-non-negative-matrix-factorization-nmf-for-sparse-matrix\n",
    "\n",
    "How to handle sparse data for NMF:\n",
    "- https://jmlr.csail.mit.edu/papers/volume5/hoyer04a/hoyer04a.pdf\n",
    "- https://faculty.cc.gatech.edu/~hpark/papers/GT-CSE-08-01.pdf"
   ],
   "id": "43af5823660e6763"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
